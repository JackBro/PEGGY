// Let's implement UDWT in CUDA now, then we will HAVE to write all the other
// pieces to go with it, for example the mid-point sorting.

// OK, this code does use convolution, on a ROTATED array, not an extended or
// padded array.  In fact, it uses FOUR convolutions, all largely the same - if
// I can remember what "RotateX s 0" in the Haskell code means compared to
// "RotateX s (0 - len)" and the other rotation values.

// There are two halves to this code, so let's do them one at a time.  The two
// halves are "MRDWT" and "MIRDWT", I'll call them "The Man" and "The Station"
// to better differentiate them ("Mr" and "Mir").  "Man" and "Machine" would
// have been better in retrospect, but I've already written several comments now
// - there's no way I can change it at this point...  Screw it, done!

// These filters were statically generated by Matlab, from which they originate.
/*float const
	gc_fForward1[] = {-0.0106,     -0.0329,      0.0308    ,  0.1870    , -0.0280    , -0.6309    ,  0.7148    , -0.2304    },
	gc_fReverse1[] = { 0.2304,      0.7148    ,  0.6309    , -0.0280    , -0.1870    ,  0.0308    ,  0.0329    , -0.0106    },
	gc_fForward2[] = {-0.0106 / 2,  0.0329 / 2,  0.0308 / 2, -0.1870 / 2, -0.0280 / 2,  0.6309 / 2,  0.7148 / 2,  0.2304 / 2},
	gc_fReverse2[] = {-0.2304 / 2,  0.7148 / 2, -0.6309 / 2, -0.0280 / 2,  0.1870 / 2,  0.0308 / 2, -0.0329 / 2, -0.0106 / 2};*/

#define CEILDIV(m,n) \
	(((m) + (n) - 1) / (n))

__device__ __constant__ float
	gc_fForward1[8],
	gc_fReverse1[8],
	gc_fForward2[8],
	gc_fReverse2[8];

extern "C" void
	UDWTCUDAFilter(
		float *     f1,
		float *     r1,
		float *     f2,
		float *     r2,
		int         len)
{
	cudaMemcpyToSymbol(gc_fForward1, f1, len * sizeof (float));
	cudaMemcpyToSymbol(gc_fReverse1, r1, len * sizeof (float));
	cudaMemcpyToSymbol(gc_fForward2, f2, len * sizeof (float));
	cudaMemcpyToSymbol(gc_fReverse2, r2, len * sizeof (float));
}

#define FILTER_SIZE                     (8)
#define WARP_SIZE                       (32) //(warpSize)
#define HALF_WARP                       (WARP_SIZE / 2)

#define EXTRA_DATA                      (((1 << 6) - 1) * (FILTER_SIZE - 1))

// ===========
// | The Man |
// ===========

void __device__
	fpconv2CUDA(
		int         i,
		float *     xIn,
		int const   shift,
		float *     filterReverse,
		float *     filterForwards,
		int         filterLength,
		float *     xOutLow,
		float *     xOutHigh)
{
	// The length of the input data needs to be a multiple of 2^level for the
	// shifts to work correctly.  I'm still sure that the original C is VERY
	// confusing and have no idea why it was ever done in that way...  OK, as a
	// result of the above statement about how confusing the original C was, I
	// went away and wrote an optimised C version which is (IMHO) much cleaner.
	// Preliminary tests don't seem to have it that much faster, however, but
	// I'm pretty sure it really should be!  Anyway, back to the CUDA matter at
	// hand...  This will be the basic implementation with no shared memory
	// code, I'll then try and write a better shared-memory version.  Also, now
	// another, much bigger, test has not come out much faster for the
	// apparently "optimised" C code, oh well.  I'll now test the effects of the
	// common sort and filter code.  Once again, back to the matter at hand -
	// less rambling procrastination!
	float
		x0 = 0.0,
		x1 = 0.0;
	int
		read = i;
	for (size_t j = filterLength; j-- > 0; )
	{
		x0 += xIn[read] * filterReverse[j];
		x1 += xIn[read] * filterForwards[j];
		// This is just the bog-standard modulo version.
		// Update: Don't need "mod" anymore because all the data exists in
		// shared memory.
		read += shift;
	}
	xOutLow[i] = x0;
	xOutHigh[i] = x1;
	// OK, I'll have to write the shared memory version from the start just to
	// resolve the data dependencies.
}

void __device__
	fpconv2NoHigh(
		int         i,
		float *     xIn,
		int const   shift,
		float *     filterReverse,
		int         filterLength,
		float *     xOutLow)
{
	float
		x0 = 0.0;
	int
		read = i;
	for (size_t j = filterLength; j-- != 0; )
	{
		x0 += xIn[read] * filterReverse[j];
		read += shift;
	}
	xOutLow[i] = x0;
}

__global__ void
	MRDWT_CUDA(
		int const   filterLength,
		int const   inputLength,
		int const   levels,
		float *     lowpassInput,
		float *     lowpassOutput,
		float *     highpassOutput)
{
	// Each level requires data from the last level beyond the edge of the data
	// this block is supposed to be calculating, and because later levels need
	// to load processed data beyond the bounds, we need even more data to be
	// able to calculate those parts too.  Fortunately this is bounded.  If we
	// have "n" levels and "n+1" stages, where stage "0" is initial data load
	// and stage "n" is final output calculation each stage ("x") GIVES this
	// much data (and requires all the data from the previous stage, with the
	// exception being stage 0):
	//  
	//  n = levels
	//  f = filterLength - 1
	//  x = currentStage
	//  m = n - x
	//  
	//  extra =  f . ((2n+1) . m + m^2)
	//          ------------------------
	//                      2
	//  
	// For now I'll call "maxLevels" 15 and enforce it in "main.cpp".  This
	// gives a stage 0 upper limit (with constant filters of size 8) of 120
	// extra elements to load initially.  For "x = 0", "n = m" and this becomes
	// the regular triangular number calculation.  For "m = 0", this is all 0.
	// 
	// OK, that calculation above seems to be WRONG!  It is:
	//  
	//  extra =  (m . m + m) . f  - 1
	//          -----------------
	//                  2
	//  
	// STILL NO!  I forgot to take in to account the fact that each level
	// DOUBLES in shift distance, it doesn't increase by one...
	//  
	//  extra = (2 ^ m - 1) . f
	//  
	// Fourth try (reversed shifting):
	//  
	//  extra = f . (((1 << n) - 1) - ((1 << m) - 1)
	//  
	__shared__ float
		fRowData1[512 + EXTRA_DATA],
		fRowData2[512 + EXTRA_DATA];
	// This is a 1D operation, so we never need to worry about pitch or higher
	// dimension index calculations.  So far this code shares a surprising
	// amount with the old convolution code, and I wasn't even trying to copy!
	// One major difference is the aligned start is not required here as long
	// as we load
	int const
		// This can be changed if you want blocks to never loop.
		blockCalc     = blockDim.x,
		f             = filterLength - 1,
		nm1           = (1 << levels) - 1,
		apronSize     = nm1 * f,
		//lIdx = threadIdx.x,
		dataStart     = blockIdx.x * blockCalc,
		//gIdx = blockIdx.x * blockDim.x + lIdx,
		//npnp1          = 2 * levels + 1,
		apronEnd      = blockCalc + apronSize;
	float * const
		highOutBase = highpassOutput + dataStart;
		//apronEndClamp = min(apronEnd, inputLength);
	// TODO: Add a top-level "if" statement to only run the edge-case code for
	// the one block that needs it.
	int
		pos  = threadIdx.x,
		load = threadIdx.x + dataStart,
		shift = 1;
	while (pos < apronEnd)
	{
		fRowData1[pos] = lowpassInput[load % inputLength];
		pos  += blockCalc;
		load += blockCalc;
	}
	// Data is preloaded pretty well.
	__syncthreads();
	float
		* localInput = fRowData1,
		* localOutput = fRowData2,
		* localSwap;
	// "- 1" because stage 0 is handled explicitly (global data loading).
	for (int level = 0, m = 1; level != levels; ++level, ++m)
	{
		const int
			procEnd = blockCalc + (nm1 - ((1 << m) - 1)) * f; //((npnp1 + m) * m * f / 2);
		pos = threadIdx.x;
		fpconv2CUDA(pos, localInput, shift, gc_fReverse1, gc_fForward1, filterLength, localOutput, highOutBase + (level * inputLength));
		pos += blockCalc;
		// This is to complete the extra local data for future loops, so it
		// doesn't write any "high" outputs.
		while (pos < procEnd)
		{
			fpconv2NoHigh(pos, localInput, shift, gc_fReverse1, filterLength, localOutput);
			pos += blockCalc;
		}
		shift <<= 1;
		localSwap = localInput;
		localInput = localOutput;
		localOutput = localSwap;
		// Now we have completed that level, we can do the next, generating
		// slightly less output data.
		__syncthreads();
	}
	// Copy from whichever output is the current one (they keep swapping).
	pos  = threadIdx.x;
	load = threadIdx.x + dataStart;
	if (levels & 1)
	{
		lowpassOutput[load] = fRowData2[pos];
	}
	else
	{
		lowpassOutput[load] = fRowData1[pos];
	}
}

extern "C" void
	DoMRDWT(
		int     filterLength,
		int     inputLength,
		int     levels,
		float * lowpassInput,
		float * lowpassOutput,
		float * highpassOutput)
{
	dim3
		// Number of blocks to execute in.
		dimBlocks(CEILDIV(inputLength, 512)),
		// Number of threads per block.
		dimThreads(512);
	MRDWT_CUDA<<<dimBlocks, dimThreads>>>(filterLength, inputLength, levels, lowpassInput, lowpassOutput, highpassOutput);
}

// ===============
// | The Machine |
// ===============

void __device__
	bpconvCUDA(
		int         i,
		int         offset,
		float *     xOut,
		int const   shift,
		float *     filterForwards,
		float *     filterReverse,
		int         filterLength,
		float *     xInLow,
		float *     xInHigh)
{
	float
		x0 = 0.0;
	int
		read = i + offset;
	for (size_t j = 0; j != filterLength; ++j)
	{
		// "xHigh" is read from global memory because every value is only ever
		// used once, so there is no advantage to caching.  Well, that may not
		// be stricktly true because the reads below are NOT aligned and may
		// slow down the main inner loop drastically!  Unfortunately, it would
		// require loading in huge amounts of global memory, equal to:
		//  
		//  (blockSize + extraSize) * levels
		//  
		// If levels becomes large, this clearly becomes largs.  It MIGHT be
		// caching the data required for each level just once - actually I think
		// it WILL be worth it because the data IS NOT read just once but
		// multiple times.  Hmm, apparently we need ALL the caching!
		x0 += xInLow[read] * filterForwards[j] + xInHigh[read] * filterReverse[j];
		read -= shift;
	}
	xOut[i] = x0;
}

__global__ void
	MIRDWT_CUDA(
		int const   filterLength,
		int const   inputLength,
		int const   levels,
		float *     lowpassInput,
		float *     lowpassOutput,
		float *     highpassInput)
{
	// TODO: Filter the inputs in parallel, not sequence.
	__shared__ float
		fHighData[512 + EXTRA_DATA],
		fRowData1[512 + EXTRA_DATA],
		fRowData2[512 + EXTRA_DATA];
	int const
		// This can be changed if you want blocks to never loop.
		blockCalc     = blockDim.x,
		f             = filterLength - 1,
		nm1           = (1 << levels) - 1,
		apronSize     = nm1 * f,
		dataStart     = blockIdx.x * blockCalc,
		dataStartBig  = dataStart + inputLength,
		apronEnd      = blockCalc + apronSize;
	// TODO: Add a top-level "if" statement to only run the edge-case code for
	// the one block that needs it.
	int
		pos  = threadIdx.x,
		load = threadIdx.x + dataStartBig - apronSize,
		shift = 1 << (levels - 1);
	while (pos < apronEnd)
	{
		fRowData1[pos] = lowpassInput[load % inputLength];
		pos  += blockCalc;
		load += blockCalc;
	}
	// Data is preloaded pretty well.
	__syncthreads();
	float
		* localInput = fRowData1,
		* localOutput = fRowData2,
		* localSwap;
	// "- 1" because stage 0 is handled explicitly (global data loading).
	for (int level = levels, m = nm1; level-- != 0; )
	{
		// Now we get to load the high-pass data in too!  Yay!
		float * const
			highIn = highpassInput + (level * inputLength);
		int const
			newApronSize = m * f,
			newApronEnd = blockCalc + newApronSize;
		m >>= 1;
		int const
			writeSize = blockCalc + m * f;
		pos  = threadIdx.x,
		load = threadIdx.x + dataStartBig - newApronSize;
		// Load in the data with a negative shift, not a positive one.
		while (pos < newApronEnd)
		{
			fHighData[pos] = highIn[load % inputLength];
			pos  += blockCalc;
			load += blockCalc;
		}
		// Now we have completed that level, we can do the next, generating
		// slightly less output data.
		__syncthreads();
		// Now we have loaded the high data, do the main calculation.
		const int
			offset = newApronEnd - writeSize;
		pos = threadIdx.x;
		bpconvCUDA(pos, offset, localOutput, shift, gc_fForward2, gc_fReverse2, filterLength, localInput, fHighData);
		// This is to complete the extra local data for future loops, so it
		// doesn't write any "high" outputs.
		pos += blockCalc;
		while (pos < writeSize)
		{
			bpconvCUDA(pos, offset, localOutput, shift, gc_fForward2, gc_fReverse2, filterLength, localInput, fHighData);
			pos += blockCalc;
		}
		shift >>= 1;
		localSwap = localInput;
		localInput = localOutput;
		localOutput = localSwap;
		// Now we have completed that level, we can do the next, generating
		// slightly less output data.
		__syncthreads();
	}
	// Copy from whichever output is the current one (they keep swapping).
	pos  = threadIdx.x;
	load = threadIdx.x + dataStart;
	if (levels & 1)
	{
		lowpassOutput[load] = fRowData2[pos];
	}
	else
	{
		lowpassOutput[load] = fRowData1[pos];
	}
}

extern "C" void
	DoMIRDWT(
		int     filterLength,
		int     inputLength,
		int     levels,
		float * lowpassInput,
		float * lowpassOutput,
		float * highpassInput)
{
	dim3
		// Number of blocks to execute in.
		dimBlocks(CEILDIV(inputLength, 512)),
		// Number of threads per block.
		dimThreads(512);
	MIRDWT_CUDA<<<dimBlocks, dimThreads>>>(filterLength, inputLength, levels, lowpassInput, lowpassOutput, highpassInput);
}
