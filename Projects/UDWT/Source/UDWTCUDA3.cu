// Let's implement UDWT in CUDA now, then we will HAVE to write all the other
// pieces to go with it, for example the mid-point sorting.

// OK, this code does use convolution, on a ROTATED array, not an extended or
// padded array.  In fact, it uses FOUR convolutions, all largely the same - if
// I can remember what "RotateX s 0" in the Haskell code means compared to
// "RotateX s (0 - len)" and the other rotation values.

// There are two halves to this code, so let's do them one at a time.  The two
// halves are "MRDWT" and "MIRDWT", I'll call them "The Man" and "The Station"
// to better differentiate them ("Mr" and "Mir").  "Man" and "Machine" would
// have been better in retrospect, but I've already written several comments now
// - there's no way I can change it at this point...  Screw it, done!

// These filters were statically generated by Matlab, from which they originate.
const float
	gc_fForward1[] = {-0.0106,     -0.0329,      0.0308    ,  0.1870    , -0.0280    , -0.6309    ,  0.7148    , -0.2304    },
	gc_fReverse1[] = { 0.2304,      0.7148    ,  0.6309    , -0.0280    , -0.1870    ,  0.0308    ,  0.0329    , -0.0106    },
	gc_fForward2[] = {-0.0106 / 2,  0.0329 / 2,  0.0308 / 2, -0.1870 / 2, -0.0280 / 2,  0.6309 / 2,  0.7148 / 2,  0.2304 / 2},
	gc_fReverse2[] = {-0.2304 / 2,  0.7148 / 2, -0.6309 / 2, -0.0280 / 2,  0.1870 / 2,  0.0308 / 2, -0.0329 / 2, -0.0106 / 2};

#define FILTER_SIZE                     (8)
#define WARP_SIZE                       (32) //(warpSize)
#define HALF_WARP                       (WARP_SIZE / 2)

// ===========
// | The Man |
// ===========

void __device__
	fpconv2CUDA(
		int         i,
		float *     xIn,
		int const   xLength,
		int const   shift,
		float *     filterReverse,
		float *     filterForwards,
		int         filterLength,
		float *     xOutLow,
		float *     xOutHigh)
{
	// The length of the input data needs to be a multiple of 2^level for the
	// shifts to work correctly.  I'm still sure that the original C is VERY
	// confusing and have no idea why it was ever done in that way...  OK, as a
	// result of the above statement about how confusing the original C was, I
	// went away and wrote an optimised C version which is (IMHO) much cleaner.
	// Preliminary tests don't seem to have it that much faster, however, but
	// I'm pretty sure it really should be!  Anyway, back to the CUDA matter at
	// hand...  This will be the basic implementation with no shared memory
	// code, I'll then try and write a better shared-memory version.  Also, now
	// another, much bigger, test has not come out much faster for the
	// apparently "optimised" C code, oh well.  I'll now test the effects of the
	// common sort and filter code.  Once again, back to the matter at hand -
	// less rambling procrastination!
	float
		x0 = 0.0,
		x1 = 0.0;
	int
		read = i;
	for (size_t j = filterLength; j-- > 0; )
	{
		x0 = x0 + xIn[read] * filterReverse[j];
		x1 = x1 + xIn[read] * filterForwards[j];
		// This is just the bog-standard modulo version.
		// Update: Don't need "mod" anymore because all the data exists in
		// shared memory.
		read += shift
	}
	xOutLow[i] = x0;
	xOutHigh[i] = x1;
	// OK, I'll have to write the shared memory version from the start just to
	// resolve the data dependencies.
}

void __device__
	fpconv2NoHigh(
		int         i,
		float *     xIn,
		int const   xLength,
		int const   shift,
		float *     filterReverse,
		int         filterLength,
		float *     xOutLow)
{
	float
		x0 = 0.0;
	int
		read = i;
	for (size_t j = filterLength; j-- > 0; )
	{
		x0 = x0 + xIn[read] * filterReverse[j];
		read += shift
	}
	xOutLow[i] = x0;
}

void
	MRDWT(
		int const   filterLength,
		int const   inputLength,
		int const   levels,
		float *     lowpassInput,
		float *     lowpassOutput,
		float *     highpassOutput)
{
	// Each level requires data from the last level beyond the edge of the data
	// this block is supposed to be calculating, and because later levels need
	// to load processed data beyond the bounds, we need even more data to be
	// able to calculate those parts too.  Fortunately this is bounded.  If we
	// have "n" levels and "n+1" stages, where stage "0" is initial data load
	// and stage "n" is final output calculation each stage ("x") GIVES this
	// much data (and requires all the data from the previous stage, with the
	// exception being stage 0):
	//  
	//  n = levels
	//  f = filterLength - 1
	//  x = currentStage
	//  m = n - x
	//  
	//  extra =  f . ((2n+1) . m + m^2)
	//          ------------------------
	//                      2
	//  
	// For now I'll call "maxLevels" 15 and enforce it in "main.cpp".  This
	// gives a stage 0 upper limit (with constant filters of size 8) of 120
	// extra elements to load initially.  For "x = 0", "n = m" and this becomes
	// the regular triangular number calculation.  For "m = 0", this is all 0.
	__shared__ float
		fRowData1[512 + 120],
		fRowData2[512 + 120];
	// This is a 1D operation, so we never need to worry about pitch or higher
	// dimension index calculations.  So far this code shares a surprising
	// amount with the old convolution code, and I wasn't even trying to copy!
	// One major difference is the aligned start is not required here as long
	// as we load
	int const
		// This can be changed if you want blocks to never loop.
		blockCalc     = blockDim.x,
		//lIdx = threadIdx.x,
		dataStart     = blockIdx.x * blockCalc,
		//gIdx = blockIdx.x * blockDim.x + lIdx,
		dataEnd       = dataStart + blockCalc,
		f             = filterLength - 1
		nnp1          = 2 * levels + 1,
		apronEnd      = dataEnd + ((levels * levels + levels) * f / 2);
	float * const
		highOutBase = highpassOutput + dataStart;
		//apronEndClamp = min(apronEnd, inputLength);
	// TODO: Add a top-level "if" statement to only run the edge-case code for
	// the one block that needs it.
	int
		pos  = threadIdx.x,
		load = threadIdx.x + dataStart,
		shift = 1;
	while (load < apronEnd)
	{
		fRowData1[pos] = lowpassInput[load % inputLength];
		pos  += blockCalc;
		load += blockCalc;
	}
	// Data is preloaded pretty well.
	__syncthreads();
	float
		* localInput = fRowData1,
		* localOutput = fRowData2,
		* localSwap;
	// "- 1" because stage 0 is handled explicitly (global data loading).
	for (int level = 0, m = levels - 1; level != levels; ++level, --m)
	{
		const int
			procEnd = blockCalc + ((nnp1 + m) * m * f / 2);
		pos = threadIdx.x;
		fpconv2CUDA(pos, localInput, inputLength, shift, gc_fReverse1, gc_fForward1, filterLength, localOutput, highOutBase + (level * inputLength));
		pos += blockCalc;
		// This is to complete the extra local data for future loops, so it
		// doesn't write any "high" outputs.
		while (pos < procEnd)
		{
			fpconv2NoHigh(pos, localInput, inputLength, shift, gc_fReverse1, filterLength, localOutput);
			pos += blockCalc;
		}
		shift <<= 1;
		localSwap = localInput;
		localInput = localOutput;
		localOutput = localSwap;
		// Now we have completed that level, we can do the next, generating
		// slightly less output data.
		__syncthreads();
	}
	// Write the local data back to global memory in the target destination.
	pos  = threadIdx.x;
	load = threadIdx.x + dataStart;
	// Copy from whichever output is the current one (they keep swapping).
	if (levels & 1)
	{
		while (load < dataEnd)
		{
			lowpassOutput[load] = fRowData2[pos];
			pos  += blockCalc;
			load += blockCalc;
		}
	}
	else
	{
		while (load < dataEnd)
		{
			lowpassOutput[load] = fRowData1[pos];
			pos  += blockCalc;
			load += blockCalc;
		}
	}
}

// ===============
// | The Machine |
// ===============
